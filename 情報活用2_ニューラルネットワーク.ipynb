{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-HATANO1970/otemae/blob/main/%E6%83%85%E5%A0%B1%E6%B4%BB%E7%94%A82_%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ニューラルネットワーク\n",
        "機械学習モデルとの比較"
      ],
      "metadata": {
        "id": "5Kfpx3WdVlu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleNeuralNetwork:\n",
        "    \"\"\"素朴なニューラルネットワーク実装（3層、ReLU活性化関数）\"\"\"\n",
        "    def __init__(self, input_size:int, hidden_size:int, output_size:int, learning_rate:float=0.01, fixed_initial_weights:bool=False):\n",
        "        \"\"\"\n",
        "        コンストラクタ\n",
        "        :param input_size: 入力層のニューロン数\n",
        "        :param hidden_size: 隠れ層のニューロン数\n",
        "        :param output_size: 出力層のニューロン数\n",
        "        :param learning_rate: 学習率\n",
        "        :param fixed_initial_weights: 初期重みを固定するかどうか\n",
        "        \"\"\"\n",
        "        # 重みの初期化\n",
        "        if(fixed_initial_weights):\n",
        "            np.random.seed(42)\n",
        "        self.W1:np.ndarray = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
        "        if(fixed_initial_weights):\n",
        "            np.random.seed(42)\n",
        "        self.W2:np.ndarray = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
        "        # バイアスの初期化\n",
        "        self.b1:np.ndarray = np.zeros((1, hidden_size))\n",
        "        self.b2:np.ndarray = np.zeros((1, output_size))\n",
        "\n",
        "        # 学習率\n",
        "        self.learning_rate: float = learning_rate\n",
        "        # 中間層の出力と活性化関数の値\n",
        "        self.z1: np.ndarray\n",
        "        self.a1: np.ndarray\n",
        "        # self.z2: np.ndarray\n",
        "        self.a2: np.ndarray\n",
        "        # 損失の履歴\n",
        "        # 学習過程での損失を記録するリスト\n",
        "        self.losses: list = []\n",
        "\n",
        "    def relu(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        ReLU活性化関数\n",
        "        負の値を0にし、正の値はそのまま返す\n",
        "        :param x: 入力データ\n",
        "        :return: ReLU適用後の出力\n",
        "        \"\"\"\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        ReLUの微分\n",
        "        :param x: 入力データ\n",
        "        :return: ReLUの微分値\n",
        "        \"\"\"\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        :param X: 入力データ (サンプル数, 入力次元)\n",
        "        :return: 出力データ (サンプル数, 出力次元)\n",
        "        \"\"\"\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        self.a2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        # self.a2 = self.z2  # 出力層は線形\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X: np.ndarray, y: np.ndarray, output: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        :param X: 入力データ (サンプル数, 入力次元)\n",
        "        :param y: 正解データ (サンプル数, 出力次元)\n",
        "        :param output: ネットワークの出力 (サンプル数, 出力次元)\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "\n",
        "        dz2 = output - y\n",
        "        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n",
        "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "        da1 = np.dot(dz2, self.W2.T)\n",
        "        dz1 = da1 * self.relu_derivative(self.z1)\n",
        "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
        "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.b2 -= self.learning_rate * db2\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "        self.b1 -= self.learning_rate * db1\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int=1000, verbose: bool=False, print_weights_every: int=10) -> None:\n",
        "        \"\"\"\n",
        "        学習\n",
        "        :param X: 入力データ (サンプル数, 入力次元)\n",
        "        :param y: 正解データ (サンプル数, 出力次元)\n",
        "        :param epochs: 学習エポック数\n",
        "        :param verbose: 学習過程を表示するかどうか\n",
        "        :param print_weights_every: 学習過程を表示する頻度\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            loss = np.mean((output - y)**2)\n",
        "            self.losses.append(loss)\n",
        "            self.accuracy = np.mean(np.round(output) == y)\n",
        "\n",
        "            if epochs > 0:\n",
        "                self.backward(X, y, output)\n",
        "\n",
        "            if verbose:\n",
        "                if 0 < epoch < 10:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "                if epoch <= 100 and epoch % print_weights_every == 0:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "                elif epoch <= 1000 and epoch % (print_weights_every*10) == 0:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "                elif epoch % (print_weights_every*50) == 0:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "                elif epoch == epochs - 1:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"予測\"\"\"\n",
        "        return self.forward(X)\n",
        "\n",
        "def print_y_vs_y_pred(y: np.ndarray, y_pred: np.ndarray) -> None:\n",
        "    \"\"\"\n",
        "    正解データと予測結果を表示\n",
        "    :param y: 正解データ\n",
        "    :param y_pred: 予測結果\n",
        "    \"\"\"\n",
        "    print(\"正解: 予測: 判定\")\n",
        "    justification = \"\"\n",
        "    y_pred_int = 0\n",
        "    correct_count = 0\n",
        "    for i in range(y_pred.size):\n",
        "        y_pred_int = int(y_pred[i].round(0))\n",
        "        # 予測結果を四捨五入して正解と比較\n",
        "        if y_pred_int == y[i]:\n",
        "            justification = \"○\"\n",
        "            correct_count += 1\n",
        "        else:\n",
        "            justification = \"×\"\n",
        "        print(f\"{y[i]}:{y_pred_int}:{justification}\")\n",
        "    print(f\"正解率: {correct_count / y.size:.2f} \")\n",
        "\n",
        "### サンプルデータと学習の実行\n",
        "'''\n",
        "X:学習時間・睡眠時間とy:不合格・合格についてのデータ\n",
        "x:学習時間・睡眠時間\n",
        "y:不合格 0, 合格 1\n",
        "'''\n",
        "X = np.array([\n",
        "    [1,10],\n",
        "    [1.5,8],\n",
        "    [2,6],\n",
        "    [3,5],\n",
        "    [3.5,7],\n",
        "    [4.5,6],\n",
        "    [5.5,7],\n",
        "    [5.8,8],\n",
        "    [6.5,7],\n",
        "    [7,5],\n",
        "    [7.5,4],\n",
        "    [8,7],\n",
        "    [8.5,6]\n",
        "])\n",
        "\n",
        "y = np.array([\n",
        "    [0],\n",
        "    [0],\n",
        "    [1],\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [0], # 1にしてみる\n",
        "    [0]  # 1にしてみる\n",
        "])\n",
        "'''\n",
        "# --- 1. シンプルなサンプルデータの準備 ---\n",
        "# 入力データ X: (サンプル数, input_size)\n",
        "# 例: ORゲートのような論理ゲートを学習\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# 正解データ y: (サンプル数, output_size)\n",
        "# 例: 論理和 (OR) の結果\n",
        "y = np.array([\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1]\n",
        "])\n",
        "'''\n",
        "# --- 2. ネットワークのパラメータ設定 ---\n",
        "input_size = X.shape[1]      # 入力データの次元 (2)\n",
        "hidden_size = 3              # 隠れ層のニューロン数 (任意に設定)\n",
        "output_size = y.shape[1]     # 出力データの次元 (1)\n",
        "learning_rate = 0.005        # 学習率\n",
        "epochs = 20000               # エポック数\n",
        "\n",
        "# --- 3. モデルの初期化と学習前の重み・バイアスの確認 ---\n",
        "print(\"--- モデル初期化直後の重みとバイアス ---\")\n",
        "model = SimpleNeuralNetwork(input_size, hidden_size, output_size, learning_rate, fixed_initial_weights=True)\n",
        "print(f\"W1 (初期):\\n{model.W1.round(4)}\")\n",
        "print(f\"b1 (初期):\\n{model.b1.round(4)}\")\n",
        "print(f\"W2 (初期):\\n{model.W2.round(4)}\")\n",
        "print(f\"b2 (初期):\\n{model.b2.round(4)}\")\n",
        "\n",
        "# --- 4. モデルの学習（途中の重み・バイアスも表示） ---\n",
        "print(\"\\n--- 学習開始 ---\")\n",
        "# verbose=Trueで損失の推移を表示\n",
        "model.fit(X, y, epochs=epochs, verbose=True)\n",
        "print(\"--- 学習終了 ---\")\n",
        "\n",
        "# --- 5. 学習後の重みとバイアスの最終確認 ---\n",
        "print(\"\\n--- 学習終了後の重みとバイアス ---\")\n",
        "print(f\"W1 (学習後):\\n{model.W1.round(4)}\")\n",
        "print(f\"b1 (学習後):\\n{model.b1.round(4)}\")\n",
        "print(f\"W2 (学習後):\\n{model.W2.round(4)}\")\n",
        "print(f\"b2 (学習後):\\n{model.b2.round(4)}\")\n",
        "\n",
        "# --- 6. 学習後の予測結果の確認 ---\n",
        "print(\"\\n--- 入力 ---\")\n",
        "print(f\"{X}\")\n",
        "print(\"\\n--- 学習後の予測結果 ---\")\n",
        "y_pred = np.array(model.predict(X))\n",
        "print(y_pred.round(2))\n",
        "print(\"\\n--- 正解と予測結果の比較 ---\")\n",
        "print_y_vs_y_pred(y.reshape(y.size), y_pred.reshape(y.size))\n",
        "\n",
        "# --- 7. 機械学習との比較 ---\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "y = y.reshape(y.size)\n",
        "model.fit(X,y)\n",
        "print(\"\\n--- 回帰式の重みとバイアス ---\")\n",
        "print(f\"W:\\n{model.coef_.round(4)}\")\n",
        "print(f\"b:\\n{model.intercept_.round(4)}\")\n",
        "y_pred = np.array(model.predict(X)).reshape(y.size)\n",
        "print(\"\\n--- 正解と予測結果の比較 ---\")\n",
        "print_y_vs_y_pred(y, y_pred)\n"
      ],
      "metadata": {
        "id": "t09YfdgmIa6H",
        "outputId": "9def7f52-be06-4ca0-dc4d-2240e7ddb1bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- モデル初期化直後の重みとバイアス ---\n",
            "W1 (初期):\n",
            "[[ 0.4967 -0.1383  0.6477]\n",
            " [ 1.523  -0.2342 -0.2341]]\n",
            "b1 (初期):\n",
            "[[0. 0. 0.]]\n",
            "W2 (初期):\n",
            "[[ 0.4056]\n",
            " [-0.1129]\n",
            " [ 0.5288]]\n",
            "b2 (初期):\n",
            "[[0.]]\n",
            "\n",
            "--- 学習開始 ---\n",
            "Epoch 0, Loss: 32.3815, Output:[[6.38 5.24 4.11 4.1  5.36 5.41 6.45 7.11 6.99 6.28 6.06 7.81 7.59]]\n",
            "Epoch 1, Loss: 0.9071, Output:[[0.7  0.57 0.44 0.47 0.54 0.84 1.06 1.07 1.35 1.64 1.86 1.79 2.01]]\n",
            "Epoch 2, Loss: 0.5444, Output:[[0.17 0.14 0.1  0.14 0.13 0.41 0.55 0.51 0.82 1.21 1.47 1.23 1.49]]\n",
            "Epoch 3, Loss: 0.5166, Output:[[0.06 0.04 0.03 0.06 0.04 0.31 0.43 0.37 0.69 1.1  1.37 1.09 1.35]]\n",
            "Epoch 4, Loss: 0.5082, Output:[[0.04 0.03 0.02 0.04 0.03 0.28 0.4  0.34 0.66 1.07 1.34 1.05 1.31]]\n",
            "Epoch 5, Loss: 0.5014, Output:[[0.05 0.03 0.02 0.03 0.03 0.28 0.39 0.33 0.65 1.05 1.32 1.03 1.3 ]]\n",
            "Epoch 6, Loss: 0.4950, Output:[[0.06 0.04 0.03 0.03 0.04 0.27 0.39 0.33 0.64 1.04 1.31 1.02 1.29]]\n",
            "Epoch 7, Loss: 0.4889, Output:[[0.07 0.05 0.04 0.03 0.05 0.27 0.39 0.33 0.64 1.04 1.3  1.01 1.28]]\n",
            "Epoch 8, Loss: 0.4831, Output:[[0.08 0.06 0.04 0.04 0.06 0.27 0.39 0.33 0.64 1.03 1.29 1.01 1.27]]\n",
            "Epoch 9, Loss: 0.4777, Output:[[0.1  0.07 0.05 0.04 0.07 0.27 0.39 0.33 0.63 1.02 1.28 1.   1.26]]\n",
            "Epoch 10, Loss: 0.4725, Output:[[0.11 0.08 0.06 0.05 0.08 0.27 0.38 0.33 0.63 1.01 1.27 1.   1.25]]\n",
            "Epoch 20, Loss: 0.4327, Output:[[0.2  0.16 0.12 0.1  0.15 0.27 0.37 0.32 0.6  0.95 1.19 0.94 1.17]]\n",
            "Epoch 30, Loss: 0.4069, Output:[[0.27 0.22 0.16 0.14 0.21 0.26 0.37 0.32 0.58 0.91 1.12 0.9  1.12]]\n",
            "Epoch 40, Loss: 0.3891, Output:[[0.32 0.26 0.2  0.17 0.25 0.26 0.36 0.32 0.56 0.87 1.07 0.87 1.07]]\n",
            "Epoch 50, Loss: 0.3721, Output:[[0.36 0.3  0.23 0.2  0.28 0.26 0.35 0.34 0.54 0.83 1.02 0.83 1.02]]\n",
            "Epoch 60, Loss: 0.3408, Output:[[0.42 0.34 0.26 0.23 0.32 0.29 0.35 0.4  0.5  0.77 0.95 0.77 0.95]]\n",
            "Epoch 70, Loss: 0.3099, Output:[[0.47 0.38 0.29 0.26 0.36 0.33 0.39 0.44 0.44 0.71 0.89 0.7  0.88]]\n",
            "Epoch 80, Loss: 0.2926, Output:[[0.5  0.41 0.32 0.28 0.39 0.35 0.42 0.47 0.43 0.67 0.84 0.65 0.83]]\n",
            "Epoch 90, Loss: 0.2834, Output:[[0.52 0.43 0.33 0.29 0.4  0.37 0.44 0.49 0.45 0.64 0.81 0.61 0.79]]\n",
            "Epoch 100, Loss: 0.2766, Output:[[0.54 0.44 0.34 0.3  0.42 0.38 0.45 0.51 0.47 0.61 0.78 0.58 0.75]]\n",
            "Epoch 200, Loss: 0.2651, Output:[[0.57 0.47 0.37 0.33 0.45 0.41 0.48 0.54 0.5  0.54 0.72 0.52 0.66]]\n",
            "Epoch 300, Loss: 0.2636, Output:[[0.57 0.47 0.37 0.34 0.45 0.41 0.49 0.54 0.5  0.53 0.71 0.53 0.65]]\n",
            "Epoch 400, Loss: 0.2623, Output:[[0.57 0.47 0.38 0.34 0.45 0.42 0.49 0.54 0.5  0.53 0.72 0.53 0.64]]\n",
            "Epoch 500, Loss: 0.2610, Output:[[0.57 0.48 0.38 0.35 0.46 0.42 0.49 0.54 0.5  0.52 0.72 0.52 0.64]]\n",
            "Epoch 600, Loss: 0.2598, Output:[[0.57 0.48 0.39 0.35 0.46 0.42 0.49 0.54 0.5  0.52 0.73 0.52 0.63]]\n",
            "Epoch 700, Loss: 0.2585, Output:[[0.56 0.48 0.39 0.36 0.46 0.43 0.49 0.54 0.5  0.52 0.73 0.52 0.63]]\n",
            "Epoch 800, Loss: 0.2573, Output:[[0.56 0.48 0.39 0.36 0.46 0.43 0.49 0.54 0.5  0.52 0.74 0.52 0.62]]\n",
            "Epoch 900, Loss: 0.2562, Output:[[0.56 0.48 0.4  0.37 0.46 0.43 0.49 0.53 0.5  0.52 0.74 0.52 0.62]]\n",
            "Epoch 1000, Loss: 0.2551, Output:[[0.55 0.48 0.4  0.37 0.46 0.43 0.49 0.53 0.5  0.52 0.75 0.52 0.61]]\n",
            "Epoch 1500, Loss: 0.2499, Output:[[0.54 0.48 0.42 0.39 0.47 0.44 0.49 0.52 0.5  0.51 0.78 0.51 0.59]]\n",
            "Epoch 2000, Loss: 0.2454, Output:[[0.52 0.48 0.43 0.42 0.47 0.45 0.49 0.51 0.49 0.51 0.8  0.51 0.57]]\n",
            "Epoch 2500, Loss: 0.2415, Output:[[0.51 0.48 0.45 0.44 0.47 0.46 0.49 0.5  0.49 0.5  0.83 0.5  0.56]]\n",
            "Epoch 3000, Loss: 0.2381, Output:[[0.5  0.48 0.46 0.45 0.48 0.47 0.48 0.5  0.49 0.5  0.85 0.49 0.54]]\n",
            "Epoch 3500, Loss: 0.2352, Output:[[0.49 0.48 0.47 0.47 0.48 0.48 0.48 0.49 0.49 0.5  0.87 0.49 0.52]]\n",
            "Epoch 4000, Loss: 0.2327, Output:[[0.48 0.48 0.49 0.49 0.48 0.49 0.48 0.48 0.48 0.49 0.89 0.48 0.51]]\n",
            "Epoch 4500, Loss: 0.2296, Output:[[0.47 0.49 0.5  0.5  0.49 0.49 0.48 0.48 0.48 0.5  0.9  0.48 0.49]]\n",
            "Epoch 5000, Loss: 0.2277, Output:[[0.46 0.49 0.51 0.52 0.49 0.5  0.48 0.47 0.48 0.5  0.92 0.47 0.48]]\n",
            "Epoch 5500, Loss: 0.2261, Output:[[0.45 0.48 0.52 0.53 0.49 0.5  0.48 0.46 0.47 0.51 0.94 0.47 0.48]]\n",
            "Epoch 6000, Loss: 0.2248, Output:[[0.44 0.48 0.53 0.54 0.49 0.51 0.48 0.45 0.47 0.51 0.96 0.46 0.48]]\n",
            "Epoch 6500, Loss: 0.2236, Output:[[0.43 0.48 0.53 0.55 0.49 0.51 0.48 0.45 0.47 0.52 0.97 0.45 0.48]]\n",
            "Epoch 7000, Loss: 0.2226, Output:[[0.42 0.48 0.54 0.56 0.49 0.51 0.47 0.44 0.46 0.52 0.99 0.45 0.48]]\n",
            "Epoch 7500, Loss: 0.2218, Output:[[0.42 0.48 0.55 0.57 0.49 0.52 0.47 0.43 0.46 0.53 1.   0.44 0.47]]\n",
            "Epoch 8000, Loss: 0.2210, Output:[[0.41 0.48 0.55 0.58 0.5  0.52 0.47 0.43 0.46 0.53 1.   0.44 0.47]]\n",
            "Epoch 8500, Loss: 0.2203, Output:[[0.4  0.48 0.56 0.59 0.5  0.53 0.47 0.42 0.46 0.54 1.   0.44 0.47]]\n",
            "Epoch 9000, Loss: 0.2197, Output:[[0.39 0.48 0.56 0.6  0.5  0.53 0.47 0.42 0.46 0.54 1.   0.44 0.47]]\n",
            "Epoch 9500, Loss: 0.2191, Output:[[0.39 0.48 0.57 0.6  0.5  0.53 0.47 0.42 0.45 0.55 1.   0.43 0.47]]\n",
            "Epoch 10000, Loss: 0.2186, Output:[[0.38 0.48 0.57 0.61 0.5  0.54 0.47 0.41 0.45 0.55 1.   0.43 0.48]]\n",
            "Epoch 10500, Loss: 0.2181, Output:[[0.38 0.48 0.58 0.62 0.5  0.54 0.47 0.41 0.45 0.55 1.   0.43 0.48]]\n",
            "Epoch 11000, Loss: 0.2176, Output:[[0.37 0.48 0.58 0.62 0.5  0.54 0.47 0.41 0.45 0.56 1.   0.43 0.48]]\n",
            "Epoch 11500, Loss: 0.2172, Output:[[0.36 0.48 0.59 0.63 0.5  0.55 0.47 0.4  0.45 0.56 1.   0.43 0.48]]\n",
            "Epoch 12000, Loss: 0.2169, Output:[[0.36 0.47 0.59 0.64 0.5  0.55 0.47 0.4  0.45 0.57 1.   0.42 0.48]]\n",
            "Epoch 12500, Loss: 0.2166, Output:[[0.35 0.47 0.6  0.64 0.5  0.55 0.47 0.4  0.45 0.57 1.   0.42 0.48]]\n",
            "Epoch 13000, Loss: 0.2163, Output:[[0.35 0.47 0.6  0.65 0.5  0.55 0.47 0.39 0.45 0.58 1.   0.42 0.48]]\n",
            "Epoch 13500, Loss: 0.2160, Output:[[0.34 0.47 0.6  0.65 0.5  0.56 0.47 0.39 0.45 0.58 1.   0.42 0.48]]\n",
            "Epoch 14000, Loss: 0.2157, Output:[[0.33 0.47 0.6  0.66 0.5  0.56 0.47 0.39 0.45 0.58 1.   0.42 0.48]]\n",
            "Epoch 14500, Loss: 0.2155, Output:[[0.33 0.47 0.61 0.66 0.5  0.56 0.47 0.39 0.45 0.59 1.   0.42 0.48]]\n",
            "Epoch 15000, Loss: 0.2153, Output:[[0.32 0.47 0.61 0.67 0.5  0.56 0.47 0.38 0.45 0.59 1.   0.42 0.48]]\n",
            "Epoch 15500, Loss: 0.2151, Output:[[0.32 0.47 0.61 0.67 0.51 0.56 0.47 0.38 0.45 0.59 1.   0.42 0.49]]\n",
            "Epoch 16000, Loss: 0.2150, Output:[[0.31 0.46 0.62 0.68 0.51 0.57 0.47 0.38 0.45 0.6  1.   0.42 0.49]]\n",
            "Epoch 16500, Loss: 0.2148, Output:[[0.31 0.46 0.62 0.68 0.51 0.57 0.47 0.38 0.45 0.6  1.   0.42 0.49]]\n",
            "Epoch 17000, Loss: 0.2147, Output:[[0.31 0.46 0.62 0.68 0.51 0.57 0.47 0.38 0.45 0.6  1.   0.42 0.49]]\n",
            "Epoch 17500, Loss: 0.2146, Output:[[0.3  0.46 0.62 0.69 0.51 0.57 0.47 0.37 0.45 0.6  1.   0.41 0.49]]\n",
            "Epoch 18000, Loss: 0.2145, Output:[[0.3  0.46 0.62 0.69 0.51 0.57 0.47 0.37 0.45 0.61 1.   0.41 0.49]]\n",
            "Epoch 18500, Loss: 0.2144, Output:[[0.29 0.46 0.62 0.69 0.51 0.57 0.47 0.37 0.44 0.61 1.   0.41 0.49]]\n",
            "Epoch 19000, Loss: 0.2143, Output:[[0.29 0.46 0.63 0.69 0.51 0.57 0.47 0.37 0.44 0.61 1.   0.41 0.49]]\n",
            "Epoch 19500, Loss: 0.2142, Output:[[0.29 0.46 0.63 0.7  0.51 0.58 0.47 0.37 0.44 0.61 1.   0.41 0.49]]\n",
            "Epoch 19999, Loss: 0.2142, Output:[[0.29 0.46 0.63 0.7  0.51 0.58 0.47 0.37 0.44 0.62 1.   0.41 0.49]]\n",
            "--- 学習終了 ---\n",
            "\n",
            "--- 学習終了後の重みとバイアス ---\n",
            "W1 (学習後):\n",
            "[[ 0.3367 -0.1383  0.3887]\n",
            " [ 1.474  -0.2342 -0.6091]]\n",
            "b1 (学習後):\n",
            "[[-0.0234  0.      0.0553]]\n",
            "W2 (学習後):\n",
            "[[-0.0619]\n",
            " [-0.1129]\n",
            " [ 0.5676]]\n",
            "b2 (学習後):\n",
            "[[1.2174]]\n",
            "\n",
            "--- 入力 ---\n",
            "[[ 1.  10. ]\n",
            " [ 1.5  8. ]\n",
            " [ 2.   6. ]\n",
            " [ 3.   5. ]\n",
            " [ 3.5  7. ]\n",
            " [ 4.5  6. ]\n",
            " [ 5.5  7. ]\n",
            " [ 5.8  8. ]\n",
            " [ 6.5  7. ]\n",
            " [ 7.   5. ]\n",
            " [ 7.5  4. ]\n",
            " [ 8.   7. ]\n",
            " [ 8.5  6. ]]\n",
            "\n",
            "--- 学習後の予測結果 ---\n",
            "[[0.29]\n",
            " [0.46]\n",
            " [0.63]\n",
            " [0.7 ]\n",
            " [0.51]\n",
            " [0.58]\n",
            " [0.47]\n",
            " [0.37]\n",
            " [0.44]\n",
            " [0.62]\n",
            " [1.  ]\n",
            " [0.41]\n",
            " [0.49]]\n",
            "\n",
            "--- 正解と予測結果の比較 ---\n",
            "正解: 予測: 判定\n",
            "0:0:○\n",
            "0:0:○\n",
            "1:1:○\n",
            "0:1:×\n",
            "1:1:○\n",
            "1:1:○\n",
            "1:0:×\n",
            "1:0:×\n",
            "0:0:○\n",
            "1:1:○\n",
            "1:1:○\n",
            "0:0:○\n",
            "0:0:○\n",
            "正解率: 0.77 \n",
            "\n",
            "--- 回帰式の重みとバイアス ---\n",
            "W:\n",
            "[[-0.0642 -0.4746]]\n",
            "b:\n",
            "[3.6236]\n",
            "\n",
            "--- 正解と予測結果の比較 ---\n",
            "正解: 予測: 判定\n",
            "0:0:○\n",
            "0:0:○\n",
            "1:1:○\n",
            "0:1:×\n",
            "1:1:○\n",
            "1:1:○\n",
            "1:0:×\n",
            "1:0:×\n",
            "0:0:○\n",
            "1:1:○\n",
            "1:1:○\n",
            "0:0:○\n",
            "0:1:×\n",
            "正解率: 0.69 \n"
          ]
        }
      ]
    }
  ]
}
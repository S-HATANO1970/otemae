{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-HATANO1970/otemae/blob/main/%E6%83%85%E5%A0%B1%E6%B4%BB%E7%94%A82_%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ニューラルネットワーク\n",
        "機械学習モデルとの比較"
      ],
      "metadata": {
        "id": "5Kfpx3WdVlu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleNeuralNetwork:\n",
        "    \"\"\"素朴なニューラルネットワーク実装（3層、ReLU活性化関数）\"\"\"\n",
        "    def __init__(self, input_size:int, hidden_size:int, output_size:int, learning_rate:float=0.01, fixed_initial_weights:bool=False):\n",
        "        \"\"\"\n",
        "        コンストラクタ\n",
        "        :param input_size: 入力層のニューロン数\n",
        "        :param hidden_size: 隠れ層のニューロン数\n",
        "        :param output_size: 出力層のニューロン数\n",
        "        :param learning_rate: 学習率\n",
        "        :param fixed_initial_weights: 初期重みを固定するかどうか\n",
        "        \"\"\"\n",
        "\n",
        "        # 重みの初期化\n",
        "        if(fixed_initial_weights):\n",
        "            np.random.seed(42)\n",
        "        self.W1:np.ndarray = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
        "        if(fixed_initial_weights):\n",
        "            np.random.seed(0)\n",
        "        self.W2:np.ndarray = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
        "        # バイアスの初期化\n",
        "        self.b1:np.ndarray = np.zeros((1, hidden_size))\n",
        "        self.b2:np.ndarray = np.zeros((1, output_size))\n",
        "\n",
        "        # 学習率\n",
        "        self.learning_rate: float = learning_rate\n",
        "        # 中間層の出力と活性化関数の値\n",
        "        self.z1: np.ndarray\n",
        "        self.a1: np.ndarray\n",
        "        self.z2: np.ndarray\n",
        "        self.a2: np.ndarray\n",
        "        # 損失の履歴\n",
        "        # 学習過程での損失を記録するリスト\n",
        "        self.losses: list = []\n",
        "\n",
        "    def relu(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        ReLU活性化関数\n",
        "        負の値を0にし、正の値はそのまま返す\n",
        "        :param x: 入力データ\n",
        "        :return: ReLU適用後の出力\n",
        "        \"\"\"\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        ReLUの微分\n",
        "        :param x: 入力データ\n",
        "        :return: ReLUの微分値\n",
        "        \"\"\"\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        :param X: 入力データ (サンプル数, 入力次元)\n",
        "        :return: 出力データ (サンプル数, 出力次元)\n",
        "        \"\"\"\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.z2  # 出力層は線形\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X: np.ndarray, y: np.ndarray, output: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        :param X: 入力データ (サンプル数, 入力次元)\n",
        "        :param y: 正解データ (サンプル数, 出力次元)\n",
        "        :param output: ネットワークの出力 (サンプル数, 出力次元)\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "\n",
        "        dz2 = output - y\n",
        "        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n",
        "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "        da1 = np.dot(dz2, self.W2.T)\n",
        "        dz1 = da1 * self.relu_derivative(self.z1)\n",
        "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
        "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.b2 -= self.learning_rate * db2\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "        self.b1 -= self.learning_rate * db1\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int=1000, verbose: bool=False, print_weights_every: int=10) -> None:\n",
        "        \"\"\"\n",
        "        学習\n",
        "        :param X: 入力データ (サンプル数, 入力次元)\n",
        "        :param y: 正解データ (サンプル数, 出力次元)\n",
        "        :param epochs: 学習エポック数\n",
        "        :param verbose: 学習過程を表示するかどうか\n",
        "        :param print_weights_every: 学習過程を表示する頻度\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            loss = np.mean((output - y)**2)\n",
        "            self.losses.append(loss)\n",
        "            self.accuracy = np.mean(np.round(output) == y)\n",
        "\n",
        "            if epochs > 0:\n",
        "                self.backward(X, y, output)\n",
        "\n",
        "            if verbose:\n",
        "                if epoch < 10:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "                if epoch <= 100 and epoch % print_weights_every == 0:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "                elif epoch <= 1000 and epoch % (print_weights_every*10) == 0:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "                elif epoch % (print_weights_every*50) == 0:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "                elif epoch == epochs - 1:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"予測\"\"\"\n",
        "        return self.forward(X)\n",
        "\n",
        "### サンプルデータと学習の実行\n",
        "'''\n",
        "X:湿度・気圧とy:天候についてのデータ\n",
        "x:湿度・気圧\n",
        "y:雨 0, 晴れ 1\n",
        "'''\n",
        "X = np.array([\n",
        "    [1, 1],\n",
        "    [1.5,12],\n",
        "    [2,6],\n",
        "    [3,1.5],\n",
        "    [3.5,10],\n",
        "    [4.5,3],\n",
        "    [5.5,4],\n",
        "    [5.8,1],\n",
        "    [6.5,7],\n",
        "    [7,11],\n",
        "    [7.5,4],\n",
        "    [8,1],\n",
        "    [8.5,12]\n",
        "])\n",
        "\n",
        "y = np.array([\n",
        "    [0],\n",
        "    [1],\n",
        "    [0],\n",
        "    [0],\n",
        "    [1],\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [0],\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1]\n",
        "])\n",
        "'''\n",
        "# --- 1. シンプルなサンプルデータの準備 ---\n",
        "# 入力データ X: (サンプル数, input_size)\n",
        "# 例: ORゲートのような論理ゲートを学習\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# 正解データ y: (サンプル数, output_size)\n",
        "# 例: 論理和 (OR) の結果\n",
        "y = np.array([\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1]\n",
        "])\n",
        "'''\n",
        "# --- 2. ネットワークのパラメータ設定 ---\n",
        "input_size = X.shape[1]      # 入力データの次元 (2)\n",
        "hidden_size = 3              # 隠れ層のニューロン数 (任意に設定)\n",
        "output_size = y.shape[1]     # 出力データの次元 (1)\n",
        "learning_rate = 0.005        # 学習率\n",
        "epochs = 20000               # エポック数\n",
        "\n",
        "# --- 3. モデルの初期化と学習前の重み・バイアスの確認 ---\n",
        "print(\"--- モデル初期化直後の重みとバイアス ---\")\n",
        "model = SimpleNeuralNetwork(input_size, hidden_size, output_size, learning_rate, fixed_initial_weights=True)\n",
        "print(f\"W1 (初期):\\n{model.W1.round(4)}\")\n",
        "print(f\"b1 (初期):\\n{model.b1.round(4)}\")\n",
        "print(f\"W2 (初期):\\n{model.W2.round(4)}\")\n",
        "print(f\"b2 (初期):\\n{model.b2.round(4)}\")\n",
        "\n",
        "# --- 4. モデルの学習（途中の重み・バイアスも表示） ---\n",
        "print(\"\\n--- 学習開始 ---\")\n",
        "# verbose=Trueで損失の推移を表示\n",
        "model.fit(X, y, epochs=epochs, verbose=True)\n",
        "print(\"--- 学習終了 ---\")\n",
        "\n",
        "# --- 5. 学習後の重みとバイアスの最終確認 ---\n",
        "print(\"\\n--- 学習終了後の重みとバイアス ---\")\n",
        "print(f\"W1 (学習後):\\n{model.W1.round(4)}\")\n",
        "print(f\"b1 (学習後):\\n{model.b1.round(4)}\")\n",
        "print(f\"W2 (学習後):\\n{model.W2.round(4)}\")\n",
        "print(f\"b2 (学習後):\\n{model.b2.round(4)}\")\n",
        "\n",
        "# --- 6. 学習後の予測結果の確認 ---\n",
        "print(\"\\n--- 学習後の予測結果 ---\")\n",
        "predictions = model.predict(X)\n",
        "print(f\"入力 X:\\n{X}\")\n",
        "print(f\"正解 y:\\n{y}\")\n",
        "print(f\"予測結果:\\n{predictions.round(0)}\")\n",
        "print(f\"精度: {model.accuracy:.2f} (正解率)\")\n",
        "\n",
        "# --- 7. 機械学習との比較 ---\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "y = y.reshape(y.size)\n",
        "model.fit(X,y)\n",
        "print(\"\\n--- 回帰式の重みとバイアス ---\")\n",
        "print(f\"W:\\n{model.coef_.round(4)}\")\n",
        "print(f\"b:\\n{model.intercept_.round(4)}\")\n",
        "y_pred = np.array(model.predict(X)).reshape(y.size,1)\n",
        "print(f\"--- 回帰式予測結果 ---\\n{y_pred}\")\n",
        "print(y_pred == y.reshape(y.size,1))\n",
        "print(f\"精度: {np.mean(y_pred == y.reshape(y.size,1)):.2f} (正解率)\")"
      ],
      "metadata": {
        "id": "t09YfdgmIa6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2c6539-7488-41f4-a4a1-14a1c0b1473b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- モデル初期化直後の重みとバイアス ---\n",
            "W1 (初期):\n",
            "[[ 0.4967 -0.1383  0.6477]\n",
            " [ 1.523  -0.2342 -0.2341]]\n",
            "b1 (初期):\n",
            "[[0. 0. 0.]]\n",
            "W2 (初期):\n",
            "[[1.4403]\n",
            " [0.3267]\n",
            " [0.7991]]\n",
            "b2 (初期):\n",
            "[[0.]]\n",
            "\n",
            "--- 学習開始 ---\n",
            "Epoch 0, Loss: 372.6505, Output:[[ 3.24 27.4  14.59  6.71 24.44 11.57 14.81  9.16 22.06 30.7  17.27 11.87\n",
            "  34.56]]\n",
            "Epoch 0, Loss: 372.6505, Output:[[ 3.24 27.4  14.59  6.71 24.44 11.57 14.81  9.16 22.06 30.7  17.27 11.87\n",
            "  34.56]]\n",
            "Epoch 1, Loss: 0.2228, Output:[[-0.03  1.22  0.51  0.01  0.92  0.08  0.16  0.72  0.47  0.92  0.17  1.17\n",
            "   0.98]]\n",
            "Epoch 2, Loss: 0.2186, Output:[[-0.03  1.2   0.5   0.01  0.91  0.08  0.16  0.73  0.47  0.9   0.18  1.18\n",
            "   0.97]]\n",
            "Epoch 3, Loss: 0.2149, Output:[[-0.03  1.19  0.5   0.01  0.9   0.08  0.16  0.73  0.46  0.89  0.18  1.19\n",
            "   0.95]]\n",
            "Epoch 4, Loss: 0.2115, Output:[[-0.03  1.17  0.49  0.02  0.88  0.07  0.15  0.74  0.45  0.88  0.19  1.2\n",
            "   0.94]]\n",
            "Epoch 5, Loss: 0.2085, Output:[[-0.03  1.16  0.48  0.02  0.87  0.07  0.15  0.74  0.44  0.87  0.2   1.21\n",
            "   0.93]]\n",
            "Epoch 6, Loss: 0.2057, Output:[[-0.03  1.14  0.48  0.02  0.86  0.07  0.15  0.75  0.44  0.85  0.2   1.21\n",
            "   0.91]]\n",
            "Epoch 7, Loss: 0.2032, Output:[[-0.03  1.13  0.47  0.03  0.85  0.07  0.14  0.76  0.43  0.84  0.21  1.22\n",
            "   0.9 ]]\n",
            "Epoch 8, Loss: 0.2009, Output:[[-0.03  1.11  0.46  0.03  0.84  0.07  0.14  0.76  0.43  0.83  0.22  1.23\n",
            "   0.89]]\n",
            "Epoch 9, Loss: 0.1988, Output:[[-0.03  1.1   0.46  0.03  0.83  0.07  0.14  0.77  0.42  0.83  0.22  1.24\n",
            "   0.88]]\n",
            "Epoch 10, Loss: 0.1969, Output:[[-0.04  1.09  0.45  0.03  0.82  0.06  0.14  0.77  0.42  0.82  0.23  1.24\n",
            "   0.87]]\n",
            "Epoch 20, Loss: 0.1841, Output:[[-0.04  1.01  0.42  0.06  0.76  0.05  0.12  0.82  0.38  0.76  0.29  1.3\n",
            "   0.81]]\n",
            "Epoch 30, Loss: 0.1777, Output:[[-0.04  0.97  0.4   0.08  0.73  0.05  0.12  0.85  0.37  0.72  0.34  1.35\n",
            "   0.78]]\n",
            "Epoch 40, Loss: 0.1740, Output:[[-0.04  0.94  0.39  0.1   0.71  0.05  0.11  0.88  0.36  0.71  0.38  1.38\n",
            "   0.76]]\n",
            "Epoch 50, Loss: 0.1716, Output:[[-0.04  0.92  0.38  0.11  0.7   0.05  0.11  0.89  0.35  0.7   0.42  1.4\n",
            "   0.75]]\n",
            "Epoch 60, Loss: 0.1701, Output:[[-0.04  0.91  0.38  0.12  0.69  0.06  0.12  0.9   0.35  0.69  0.44  1.41\n",
            "   0.74]]\n",
            "Epoch 70, Loss: 0.1690, Output:[[-0.04  0.9   0.37  0.13  0.68  0.08  0.12  0.91  0.35  0.69  0.46  1.42\n",
            "   0.74]]\n",
            "Epoch 80, Loss: 0.1680, Output:[[-0.04  0.9   0.37  0.14  0.68  0.09  0.12  0.91  0.35  0.68  0.47  1.42\n",
            "   0.73]]\n",
            "Epoch 90, Loss: 0.1672, Output:[[-0.03  0.89  0.37  0.14  0.68  0.1   0.12  0.91  0.35  0.68  0.48  1.42\n",
            "   0.73]]\n",
            "Epoch 100, Loss: 0.1664, Output:[[-0.03  0.89  0.37  0.15  0.67  0.1   0.12  0.91  0.35  0.68  0.49  1.42\n",
            "   0.73]]\n",
            "Epoch 200, Loss: 0.1554, Output:[[-0.03  0.87  0.36  0.19  0.66  0.18  0.18  0.92  0.36  0.68  0.59  1.42\n",
            "   0.73]]\n",
            "Epoch 300, Loss: 0.1340, Output:[[-0.04  0.83  0.34  0.24  0.63  0.3   0.34  0.95  0.34  0.65  0.76  1.44\n",
            "   0.7 ]]\n",
            "Epoch 400, Loss: 0.1245, Output:[[-0.06  0.83  0.33  0.24  0.63  0.34  0.4   0.9   0.33  0.64  0.8   1.38\n",
            "   0.7 ]]\n",
            "Epoch 500, Loss: 0.1184, Output:[[-0.08  0.83  0.32  0.24  0.62  0.36  0.44  0.87  0.31  0.64  0.82  1.33\n",
            "   0.69]]\n",
            "Epoch 600, Loss: 0.1167, Output:[[-0.09  0.84  0.32  0.22  0.63  0.34  0.43  0.83  0.31  0.64  0.79  1.28\n",
            "   0.7 ]]\n",
            "Epoch 700, Loss: 0.1164, Output:[[-0.1   0.85  0.32  0.21  0.63  0.33  0.42  0.82  0.31  0.64  0.78  1.26\n",
            "   0.7 ]]\n",
            "Epoch 800, Loss: 0.1162, Output:[[-0.1   0.85  0.31  0.2   0.63  0.33  0.42  0.81  0.31  0.64  0.78  1.26\n",
            "   0.7 ]]\n",
            "Epoch 900, Loss: 0.1161, Output:[[-0.11  0.85  0.31  0.2   0.63  0.33  0.41  0.81  0.3   0.64  0.78  1.26\n",
            "   0.7 ]]\n",
            "Epoch 1000, Loss: 0.1160, Output:[[-0.11  0.86  0.31  0.2   0.63  0.32  0.41  0.81  0.3   0.65  0.78  1.26\n",
            "   0.7 ]]\n",
            "Epoch 1500, Loss: 0.1156, Output:[[-0.13  0.86  0.3   0.18  0.63  0.32  0.4   0.81  0.3   0.65  0.78  1.27\n",
            "   0.7 ]]\n",
            "Epoch 2000, Loss: 0.1154, Output:[[-0.14  0.86  0.3   0.17  0.63  0.31  0.4   0.81  0.29  0.65  0.78  1.28\n",
            "   0.71]]\n",
            "Epoch 2500, Loss: 0.1153, Output:[[-0.15  0.86  0.29  0.17  0.63  0.31  0.4   0.81  0.29  0.65  0.78  1.28\n",
            "   0.71]]\n",
            "Epoch 3000, Loss: 0.1152, Output:[[-0.16  0.86  0.29  0.16  0.63  0.3   0.4   0.81  0.29  0.65  0.79  1.28\n",
            "   0.71]]\n",
            "Epoch 3500, Loss: 0.1152, Output:[[-0.16  0.86  0.29  0.16  0.63  0.3   0.4   0.81  0.29  0.65  0.79  1.29\n",
            "   0.72]]\n",
            "Epoch 4000, Loss: 0.1151, Output:[[-0.17  0.86  0.29  0.16  0.63  0.3   0.4   0.81  0.29  0.65  0.79  1.29\n",
            "   0.72]]\n",
            "Epoch 4500, Loss: 0.1151, Output:[[-0.17  0.86  0.28  0.16  0.63  0.3   0.4   0.81  0.29  0.66  0.79  1.29\n",
            "   0.72]]\n",
            "Epoch 5000, Loss: 0.1151, Output:[[-0.17  0.86  0.28  0.16  0.63  0.3   0.4   0.81  0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 5500, Loss: 0.1151, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 6000, Loss: 0.1151, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 6500, Loss: 0.1150, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 7000, Loss: 0.1150, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 7500, Loss: 0.1150, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 8000, Loss: 0.1150, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 8500, Loss: 0.1150, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 9000, Loss: 0.1150, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 9500, Loss: 0.1150, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 10000, Loss: 0.1150, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 10500, Loss: 0.1150, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 11000, Loss: 0.1149, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 11500, Loss: 0.1149, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 12000, Loss: 0.1149, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 12500, Loss: 0.1149, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 13000, Loss: 0.1149, Output:[[-0.17  0.86  0.28  0.15  0.63  0.3   0.4   0.8   0.29  0.66  0.8   1.29\n",
            "   0.72]]\n",
            "Epoch 13500, Loss: 0.1149, Output:[[-0.17  0.86  0.28  0.14  0.63  0.3   0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 14000, Loss: 0.1149, Output:[[-0.17  0.86  0.28  0.14  0.63  0.3   0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 14500, Loss: 0.1149, Output:[[-0.17  0.86  0.28  0.14  0.63  0.3   0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 15000, Loss: 0.1149, Output:[[-0.17  0.86  0.28  0.14  0.63  0.3   0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 15500, Loss: 0.1149, Output:[[-0.17  0.86  0.28  0.14  0.63  0.29  0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 16000, Loss: 0.1149, Output:[[-0.17  0.86  0.28  0.14  0.63  0.29  0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 16500, Loss: 0.1148, Output:[[-0.17  0.86  0.28  0.14  0.63  0.29  0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 17000, Loss: 0.1148, Output:[[-0.17  0.86  0.28  0.14  0.63  0.3   0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 17500, Loss: 0.1148, Output:[[-0.17  0.86  0.28  0.14  0.63  0.29  0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 18000, Loss: 0.1148, Output:[[-0.17  0.86  0.28  0.14  0.63  0.29  0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 18500, Loss: 0.1148, Output:[[-0.17  0.86  0.28  0.14  0.63  0.29  0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 19000, Loss: 0.1148, Output:[[-0.17  0.86  0.28  0.14  0.63  0.3   0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 19500, Loss: 0.1148, Output:[[-0.17  0.86  0.28  0.14  0.63  0.29  0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "Epoch 19999, Loss: 0.1148, Output:[[-0.17  0.86  0.28  0.14  0.63  0.29  0.4   0.8   0.29  0.66  0.81  1.29\n",
            "   0.72]]\n",
            "--- 学習終了 ---\n",
            "\n",
            "--- 学習終了後の重みとバイアス ---\n",
            "W1 (学習後):\n",
            "[[-0.1215 -0.1383  0.4142]\n",
            " [ 0.5814 -0.2342 -0.3646]]\n",
            "b1 (学習後):\n",
            "[[-0.0276  0.     -0.1396]]\n",
            "W2 (学習後):\n",
            "[[0.1627]\n",
            " [0.3267]\n",
            " [0.5456]]\n",
            "b2 (学習後):\n",
            "[[-0.2406]]\n",
            "\n",
            "--- 学習後の予測結果 ---\n",
            "入力 X:\n",
            "[[ 1.   1. ]\n",
            " [ 1.5 12. ]\n",
            " [ 2.   6. ]\n",
            " [ 3.   1.5]\n",
            " [ 3.5 10. ]\n",
            " [ 4.5  3. ]\n",
            " [ 5.5  4. ]\n",
            " [ 5.8  1. ]\n",
            " [ 6.5  7. ]\n",
            " [ 7.  11. ]\n",
            " [ 7.5  4. ]\n",
            " [ 8.   1. ]\n",
            " [ 8.5 12. ]]\n",
            "正解 y:\n",
            "[[0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "予測結果:\n",
            "[[-0.]\n",
            " [ 1.]\n",
            " [ 0.]\n",
            " [ 0.]\n",
            " [ 1.]\n",
            " [ 0.]\n",
            " [ 0.]\n",
            " [ 1.]\n",
            " [ 0.]\n",
            " [ 1.]\n",
            " [ 1.]\n",
            " [ 1.]\n",
            " [ 1.]]\n",
            "精度: 0.85 (正解率)\n",
            "\n",
            "--- 回帰式の重みとバイアス ---\n",
            "W:\n",
            "[[0.2961 0.079 ]]\n",
            "b:\n",
            "[-1.732]\n",
            "--- 回帰式予測結果 ---\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "[[ True]\n",
            " [False]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [False]\n",
            " [False]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]]\n",
            "精度: 0.77 (正解率)\n"
          ]
        }
      ]
    }
  ]
}
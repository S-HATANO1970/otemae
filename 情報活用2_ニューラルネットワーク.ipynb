{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-HATANO1970/otemae/blob/main/%E6%83%85%E5%A0%B1%E6%B4%BB%E7%94%A82_%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ニューラルネットワーク\n",
        "機械学習モデルとの比較"
      ],
      "metadata": {
        "id": "5Kfpx3WdVlu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleNeuralNetwork:\n",
        "    \"\"\"素朴なニューラルネットワーク実装（3層、ReLU活性化関数）\"\"\"\n",
        "    def __init__(self, input_size:int, hidden_size:int, output_size:int, learning_rate:float=0.01, fixed_initial_weights:bool=False):\n",
        "        \"\"\"\n",
        "        コンストラクタ\n",
        "        :param input_size: 入力層のニューロン数\n",
        "        :param hidden_size: 隠れ層のニューロン数\n",
        "        :param output_size: 出力層のニューロン数\n",
        "        :param learning_rate: 学習率\n",
        "        :param fixed_initial_weights: 初期重みを固定するかどうか\n",
        "        \"\"\"\n",
        "        # 重みの初期化\n",
        "        if(fixed_initial_weights):\n",
        "            np.random.seed(42)\n",
        "        self.W1:np.ndarray = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
        "        if(fixed_initial_weights):\n",
        "            np.random.seed(42)\n",
        "        self.W2:np.ndarray = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
        "        # バイアスの初期化\n",
        "        self.b1:np.ndarray = np.zeros((1, hidden_size))\n",
        "        self.b2:np.ndarray = np.zeros((1, output_size))\n",
        "\n",
        "        # 学習率\n",
        "        self.learning_rate: float = learning_rate\n",
        "        # 中間層の出力と活性化関数の値\n",
        "        self.z1: np.ndarray\n",
        "        self.a1: np.ndarray\n",
        "        # self.z2: np.ndarray\n",
        "        self.a2: np.ndarray\n",
        "        # 損失の履歴\n",
        "        # 学習過程での損失を記録するリスト\n",
        "        self.losses: list = []\n",
        "\n",
        "    def relu(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        ReLU活性化関数\n",
        "        負の値を0にし、正の値はそのまま返す\n",
        "        :param x: 入力データ\n",
        "        :return: ReLU適用後の出力\n",
        "        \"\"\"\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        ReLUの微分\n",
        "        :param x: 入力データ\n",
        "        :return: ReLUの微分値\n",
        "        \"\"\"\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        :param X: 入力データ (サンプル数, 入力次元)\n",
        "        :return: 出力データ (サンプル数, 出力次元)\n",
        "        \"\"\"\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        self.a2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        # self.a2 = self.z2  # 出力層は線形\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X: np.ndarray, y: np.ndarray, output: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        :param X: 入力データ (サンプル数, 入力次元)\n",
        "        :param y: 正解データ (サンプル数, 出力次元)\n",
        "        :param output: ネットワークの出力 (サンプル数, 出力次元)\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "\n",
        "        dz2 = output - y\n",
        "        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n",
        "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "        da1 = np.dot(dz2, self.W2.T)\n",
        "        dz1 = da1 * self.relu_derivative(self.z1)\n",
        "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
        "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.b2 -= self.learning_rate * db2\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "        self.b1 -= self.learning_rate * db1\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int=1000, verbose: bool=False, print_weights_every: int=10) -> None:\n",
        "        \"\"\"\n",
        "        学習\n",
        "        :param X: 入力データ (サンプル数, 入力次元)\n",
        "        :param y: 正解データ (サンプル数, 出力次元)\n",
        "        :param epochs: 学習エポック数\n",
        "        :param verbose: 学習過程を表示するかどうか\n",
        "        :param print_weights_every: 学習過程を表示する頻度\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            loss = np.mean((output - y)**2)\n",
        "            self.losses.append(loss)\n",
        "            self.accuracy = np.mean(np.round(output) == y)\n",
        "\n",
        "            if epochs > 0:\n",
        "                self.backward(X, y, output)\n",
        "\n",
        "            if verbose:\n",
        "                if 0 < epoch < 10:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "                if epoch <= 100 and epoch % print_weights_every == 0:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "                elif epoch <= 1000 and epoch % (print_weights_every*10) == 0:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "                elif epoch % (print_weights_every*50) == 0:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "                elif epoch == epochs - 1:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Output:{output.reshape(1,y.size).round(2)}\")\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"予測\"\"\"\n",
        "        return self.forward(X)\n",
        "\n",
        "def print_y_vs_y_pred(y: np.ndarray, y_pred: np.ndarray) -> None:\n",
        "    \"\"\"\n",
        "    正解データと予測結果を表示\n",
        "    :param y: 正解データ\n",
        "    :param y_pred: 予測結果\n",
        "    \"\"\"\n",
        "    print(\"正解: 予測: 判定\")\n",
        "    justification = \"\"\n",
        "    y_pred_int = 0\n",
        "    correct_count = 0\n",
        "    for i in range(y_pred.size):\n",
        "        y_pred_int = int(y_pred[i].round(0))\n",
        "        # 予測結果を四捨五入して正解と比較\n",
        "        if y_pred_int == y[i]:\n",
        "            justification = \"○\"\n",
        "            correct_count += 1\n",
        "        else:\n",
        "            justification = \"×\"\n",
        "        print(f\"{y[i]}:{y_pred_int}:{justification}\")\n",
        "    print(f\"正解率: {correct_count / y.size:.2f} \")\n",
        "\n",
        "### サンプルデータと学習の実行\n",
        "'''\n",
        "X:学習時間・睡眠時間とy:不合格・合格についてのデータ\n",
        "x:学習時間・睡眠時間\n",
        "y:不合格 0, 合格 1\n",
        "'''\n",
        "X = np.array([\n",
        "    [1,10],\n",
        "    [1.5,8],\n",
        "    [2,6],\n",
        "    [3,5],\n",
        "    [3.5,7],\n",
        "    [4.5,6],\n",
        "    [5.5,7],\n",
        "    [5.8,8],\n",
        "    [6.5,7],\n",
        "    [7,5],\n",
        "    [7.5,4],\n",
        "    [8,7],\n",
        "    [8.5,6]\n",
        "])\n",
        "\n",
        "y = np.array([\n",
        "    [0],\n",
        "    [0],\n",
        "    [1],\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [0],  # 1にしてみる\n",
        "    [0]   # 1にしてみる\n",
        "])\n",
        "'''\n",
        "# --- 1. シンプルなサンプルデータの準備 ---\n",
        "# 入力データ X: (サンプル数, input_size)\n",
        "# 例: ORゲートのような論理ゲートを学習\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# 正解データ y: (サンプル数, output_size)\n",
        "# 例: 論理和 (OR) の結果\n",
        "y = np.array([\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1]\n",
        "])\n",
        "'''\n",
        "# --- 2. ネットワークのパラメータ設定 ---\n",
        "input_size = X.shape[1]      # 入力データの次元 (2)\n",
        "hidden_size = 3              # 隠れ層のニューロン数 (任意に設定)\n",
        "output_size = y.shape[1]     # 出力データの次元 (1)\n",
        "learning_rate = 0.005        # 学習率\n",
        "epochs = 20000               # エポック数\n",
        "\n",
        "# --- 3. モデルの初期化と学習前の重み・バイアスの確認 ---\n",
        "print(\"--- モデル初期化直後の重みとバイアス ---\")\n",
        "model = SimpleNeuralNetwork(input_size, hidden_size, output_size, learning_rate, fixed_initial_weights=True)\n",
        "print(f\"W1 (初期):\\n{model.W1.round(4)}\")\n",
        "print(f\"b1 (初期):\\n{model.b1.round(4)}\")\n",
        "print(f\"W2 (初期):\\n{model.W2.round(4)}\")\n",
        "print(f\"b2 (初期):\\n{model.b2.round(4)}\")\n",
        "\n",
        "# --- 4. モデルの学習（途中の重み・バイアスも表示） ---\n",
        "print(\"\\n--- 学習開始 ---\")\n",
        "# verbose=Trueで損失の推移を表示\n",
        "model.fit(X, y, epochs=epochs, verbose=True)\n",
        "print(\"--- 学習終了 ---\")\n",
        "\n",
        "# --- 5. 学習後の重みとバイアスの最終確認 ---\n",
        "print(\"\\n--- 学習終了後の重みとバイアス ---\")\n",
        "print(f\"W1 (学習後):\\n{model.W1.round(4)}\")\n",
        "print(f\"b1 (学習後):\\n{model.b1.round(4)}\")\n",
        "print(f\"W2 (学習後):\\n{model.W2.round(4)}\")\n",
        "print(f\"b2 (学習後):\\n{model.b2.round(4)}\")\n",
        "\n",
        "# --- 6. 学習後の予測結果の確認 ---\n",
        "print(\"\\n--- 入力 ---\")\n",
        "print(f\"{X}\")\n",
        "print(\"\\n--- 学習後の予測結果 ---\")\n",
        "y_pred = np.array(model.predict(X))\n",
        "print(y_pred.round(2))\n",
        "print(\"\\n--- 正解と予測結果の比較 ---\")\n",
        "print_y_vs_y_pred(y.reshape(y.size), y_pred.reshape(y.size))\n",
        "\n",
        "# --- 7. 機械学習との比較 ---\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "y = y.reshape(y.size)\n",
        "model.fit(X,y)\n",
        "print(\"\\n--- 回帰式の重みとバイアス ---\")\n",
        "print(f\"W:\\n{model.coef_.round(4)}\")\n",
        "print(f\"b:\\n{model.intercept_.round(4)}\")\n",
        "y_pred = np.array(model.predict(X)).reshape(y.size)\n",
        "print(\"\\n--- 正解と予測結果の比較 ---\")\n",
        "print_y_vs_y_pred(y, y_pred)\n"
      ],
      "metadata": {
        "id": "t09YfdgmIa6H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
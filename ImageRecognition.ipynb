{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOG/Av/c2ugTTK2VAaletGy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Dwfcy7WnBpS","executionInfo":{"status":"ok","timestamp":1754994347237,"user_tz":-540,"elapsed":7337,"user":{"displayName":"秦野 伸介hatano shinsuke","userId":"08218132580338133724"}},"outputId":"9892e986-e2e5-4789-975e-8c0ff3715272"},"outputs":[{"output_type":"stream","name":"stdout","text":["キャプションと確率:\n","A shark stuffed animal: 0.1887\n","A Fox stuffed animal: 0.5170\n","A Triceratops stuffed animal: 0.2943\n","A person reading a book: 0.0000\n","A cup of coffee on a table: 0.0000\n","\n","最も適切なキャプション: A Fox stuffed animal (確率: 0.5170)\n"]}],"source":["# 必要なライブラリをインストール（Google Colabでは初回実行時に必要）\n","# !pip install transformers torch Pillow requests\n","\n","# ライブラリをインポート\n","from transformers import CLIPProcessor, CLIPModel\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","import torch\n","\n","# 1. CLIPモデルとプロセッサの読み込み\n","model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","# 2. 画像をURLから読み込む\n","image_url =\"https://kyoto-edu.sakura.ne.jp/althusser/img/fox.jpg\"  # ここに有効な画像URLを指定\n","try:\n","    response = requests.get(image_url)\n","    response.raise_for_status()  # URLが無効な場合にエラーを発生\n","    image = Image.open(BytesIO(response.content))\n","except requests.exceptions.RequestException as e:\n","    print(f\"Error fetching image from URL: {e}\")\n","    exit()\n","except Exception as e:\n","    print(f\"Error processing image: {e}\")\n","    exit()\n","\n","# 3. キャプション候補を定義\n","candidate_captions = [\n","    \"A shark stuffed animal\",\n","    \"A Fox stuffed animal\",\n","    \"A Triceratops stuffed animal\",\n","    \"A person reading a book\",\n","    \"A cup of coffee on a table\"\n","]\n","\n","# 4. 画像とキャプションを処理\n","inputs = processor(text=candidate_captions, images=image, return_tensors=\"pt\", padding=True)\n","\n","# 5. モデルで画像とキャプションのマッチングスコアを計算\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","    logits_per_image = outputs.logits_per_image\n","    probs = logits_per_image.softmax(dim=1)\n","\n","# 6. 結果を表示\n","print(\"キャプションと確率:\")\n","for caption, prob in zip(candidate_captions, probs[0]):\n","    print(f\"{caption}: {prob:.4f}\")\n","\n","# 7. 最も確率の高いキャプションを選択\n","best_caption_idx = probs.argmax().item()\n","best_caption = candidate_captions[best_caption_idx]\n","print(f\"\\n最も適切なキャプション: {best_caption} (確率: {probs[0][best_caption_idx]:.4f})\")\n","\n","# 8. 画像を表示（確認用、Google Colabでは表示可能）\n","image.show()"]}]}